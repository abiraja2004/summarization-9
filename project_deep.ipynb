{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Based losely on https://arxiv.org/abs/1611.01603\n",
    "#https://github.com/allenai/bi-att-flow\n",
    "#https://www.aclweb.org/anthology/D17-1151\n",
    "#https://www.oreilly.com/learning/capturing-semantic-meanings-using-deep-learning\n",
    "#https://cs224d.stanford.edu/reports/StrohMathur.pdf\n",
    "#https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
    "#https://github.com/keras-team/keras/issues/4962\n",
    "#https://gist.github.com/mbollmann/ccc735366221e4dba9f89d2aab86da1e\n",
    "#https://github.com/abisee/pointer-generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization - recursive deep LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: mehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from odo import odo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ['context','paragraphs','summary','comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Activation\n",
    "#from keras.engine.input_layer import Input\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import merge\n",
    "\n",
    "from keras.models import Model \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from attention_decoder import AttentionDecoder\n",
    "from attention_decoder import _time_distributed_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Mihai/summarization'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_names = ['name','context','summ','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Mihai/summarization/demo.train.0.pt\n"
     ]
    }
   ],
   "source": [
    "print(curr_dir+ \"/demo.train.0.pt\" )\n",
    "#parsedXML = et.parse(curr_dir+ \"/demo.train.0.pt\" )d\n",
    "df = pd.read_csv(curr_dir+ \"/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>279</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  answer_start  \\\n",
       "0           0             0           515   \n",
       "1           1             1           188   \n",
       "2           2             2           279   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                         text  \n",
       "0  Saint Bernadette Soubirous  \n",
       "1   a copper statue of Christ  \n",
       "2           the Main Building  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)\n",
    "#df.rename(index=str, columns={\"answer_start\": \"name\", \"context\": \"context\", \"question\":\"summ\"})\n",
    "#df.to_csv(curr_dir+ \"/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access_data import get_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main = get_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle_unpickle import pickle_this, unpickle_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_text import process_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbedd_this import embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokenizer = embedd(text)\n",
    "    tokenizer = [process_text(x)[0] for x in tokenizer if len(process_text(x))!=0 if x is not None]\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd_seq(text_encoded):\n",
    "    padded_docs = pad_sequences(text_encoded, maxlen=1000, padding='post')\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(texts):\n",
    "    encoded_texts = [one_hot(t[0], 1000) for t in texts]\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbedd_this import open_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbedd_this import embedd_this, embedd_with_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"mongodb://nlp:misu5002@ds163781.mlab.com:63781/reading_comprehension\"\n",
    "client = MongoClient(uri)\n",
    "#print(client.get_default_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client[\"reading_comprehension\"][\"context_question_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x1c26811a20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = open_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=1000)\n",
    "tokenizer.fit_on_texts(df.context)\n",
    "sequences = tokenizer.texts_to_sequences(df.context)\n",
    "#sequences = tokenizer.texts_to_sequences(main.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86934 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.DataFrame(columns=dfcols)\n",
    "#def getvalueofnode( node ):\n",
    "    #return node.text if node is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_model(context_length, question_length):\n",
    "def get_model():\n",
    "    max_len_context = 1000 # number of words in the text\n",
    "    max_len_question = 1000\n",
    "    max_len_answer = 1000\n",
    "    n_timesteps = 100\n",
    "    embedding_size=100#question_embedding.shape[1]\n",
    "    vocab_size= 1000#question_embedding.shape[0]#8#10000#question_embedding.shape[0]\n",
    "    units = 1\n",
    "    trainable = False\n",
    "    #masking = False\n",
    "    \n",
    "    #print(question_embedding.shape)\n",
    "    _input = Input(shape=[max_len_context], dtype='float32')#, embedding_size\n",
    "    _q     = Input(shape=[max_len_question], dtype='float32')\n",
    "    \n",
    "    c_embedded = Embedding(input_dim=86935, output_dim=embedding_size, input_length=1000, trainable=trainable,\n",
    "                         #mask_zero=masking,\n",
    "                         weights=[embedding_matrix]\n",
    "                        )(_input)\n",
    "    \n",
    "    q_embedded = Embedding(input_dim=86935, output_dim=embedding_size, input_length=1000, trainable=trainable,\n",
    "                         #mask_zero=masking,\n",
    "                         weights=[embedding_matrix]\n",
    "                        )(_q)\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    pcont = Bidirectional(LSTM(50, return_sequences=True), input_shape=(n_timesteps, 1), merge_mode='concat')(c_embedded)\n",
    "    qcont = Bidirectional(LSTM(50, return_sequences=True), input_shape=(n_timesteps, 1), merge_mode='concat')(q_embedded)\n",
    "    \n",
    "    ti = TimeDistributed(Dense(1000, activation='sigmoid'))(pcont)\n",
    "    #ti = RepeatVector(max_len_answer)(ti)\n",
    "    #attention applied on the question \n",
    "    at1 = TimeDistributed(Dense(1000, activation='sigmoid'))(qcont)\n",
    "    #at = Flatten (at)\n",
    "    at2 = AttentionDecoder(150, 1)(at1)#(qcont)\n",
    "    at3 = Activation('softmax')(at2)\n",
    "    #at = RepeatVector(units)(at)\n",
    "    #at = Permute([2, 1])(at)\n",
    "    \n",
    "    inpt = [ti, at3]\n",
    "    print(type(ti))\n",
    "    print(type(at3))\n",
    "    #applied the question attention to the paragraph\n",
    "    #qp_representation = merge(inpt, mode='mul')\n",
    "    #qp_representation = keras.layers.multiply(inpt)\n",
    "    qp_representation = keras.layers.concatenate(inpt)\n",
    "    #qp_representation2 = Flatten(qp_representation)\n",
    "    \n",
    "    #output = Dense(vocab_size, activation='softmax')(qp_representation2)\n",
    "    output = Dense(vocab_size, activation='softmax')(qp_representation)\n",
    "    print()\n",
    "    #model = Sequential(inputs=_input, outputs=output)\n",
    "    model = Model(inputs=[_input,_q], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need the correct X, y and  need question formate\n",
    "def train_model(model, n_timesteps):\n",
    "    loss = list()\n",
    "    for _ in range(250):\n",
    "        # generate new random sequence\n",
    "        #X,y = get_sequence(n_timesteps)\n",
    "        X,y=[df.context[0:n_timesteps], df.question[0:n_timesteps]],df.text[0:n_timesteps]# main.context[0:n_timesteps][main.answer_start[0:n_timesteps]:main.answer_start[0]+40]\n",
    "        #X,y=[data[0:n_timesteps], main.question[0:n_timesteps]],main.text[0:n_timesteps]# main.context[0:n_timesteps][main.answer_start[0:n_timesteps]:main.answer_start[0]+40]\n",
    "        #print('X', X)\n",
    "        #print('y', y)\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, epochs=1, batch_size=1, verbose=1)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 100)    8693500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1000, 100)    60400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    8693500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 1000, 1000)   101000      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1000, 100)    60400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AttentionDecoder (AttentionDeco (None, 1000, 1)      842352      time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1000, 1000)   101000      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1000, 1)      0           AttentionDecoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000, 1001)   0           time_distributed_1[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000, 1000)   1002000     concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 19,554,152\n",
      "Trainable params: 2,167,152\n",
      "Non-trainable params: 17,387,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[[460], [223], [576], [576], [460], [616], [517], [84], [414], [84], [223], [526], [616], [652], [124], [616], [517], [124], [576], [223], [576], [460], [70], [817], [526], [460], [310], [39], [616], [517], [517], [223], [538], [652], [517], [517], [84], [616], [246], [246], [696], [696], [84], [817], [124], [526], [616], [696], [460], [223], [517], [223], [105], [525], [616], [414], [414], [817], [576], [720], [223], [84], [414], [223], [616], [223], [223], [616]], [[526], [616], [460], [460], [105], [817], [124]]]\n",
      "y [[223], [517], [223]]\n"
     ]
    }
   ],
   "source": [
    "X,y=[preprocess_text(df.context[0]), preprocess_text(df.question[0])], preprocess_text(df.context[0][df.answer_start[0]:df.answer_start[0]+20])\n",
    "X,y=[encode_docs(X[0]), encode_docs(X[1])],encode_docs(y)\n",
    "\n",
    "print('X',X)\n",
    "print('y',y)\n",
    "#model.fit(X, y, epochs=1, batch_size=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
